## Out of bread, customer leaves = $0 profit
## Each wasted bread = -$0.20 profit
##
## WARNING. This code has a known bug. It will crash
## for certain combinations of simulation arguments.
sim.demand <- function(n){
bread <- runif(n) # Probabilities of bread choice
ifelse(bread <= 0.5, 'white',
ifelse(bread <= 0.75, 'wheat', 'multi'))
}
demand <- sim.demand(100)
table(demand)
sim.profit <- function(n = 100, sd = 30, bake = 120, test = FALSE){
# number of bread by type
baked <- c(rep('white', times = bake/2),
rep('wheat', times = bake/4),
rep('multi', times = bake/4))
baked <- as.data.frame(table(baked))
names(baked) <- c('bread', 'baked')
arrivals <- 0
while(arrivals < 1) arrivals <- rnorm(1, mean = n, sd = sd)
demand <- sim.demand(arrivals) # demand by bread type
baked$demand <- as.data.frame(table(demand))[,2]
baked$shortfall <- baked$demand - baked$baked
baked$profit <- baked$demand -
ifelse(baked$shortfall < 0, 0.2 * -baked$shortfall,
baked$shortfall )
if(test) print(baked)
data.frame(profit = sum(baked$profit), missed = sum(baked$shortfall))
}
sim.profit(test = TRUE)
plot.profit <- function(df, bins = 50){
require(ggplot2)
require(gridExtra)
bw <- (max(df$profit) - min(df$profit))/(bins - 1)
h1 <- ggplot(df, aes(profit)) + geom_histogram(binwidth = bw) +
ggtitle('Distributions of profits') + xlab('Profits')
bw <- (max(df$missed) - min(df$missed))/(bins - 1)
h2 <- ggplot(df, aes(missed)) + geom_histogram(binwidth = bw) +
ggtitle('Distributions of people missed') + xlab('Missed')
grid.arrange(h1, h2, nrow = 1)
}
dist.profit <- function(reps = 1000, n = 100, sd = 20, bake = 120){
dist <- data.frame(profit = rep(0, times = reps),
missed = rep(0, times = reps))
for(i in 1:reps){
dist[i, ] <- sim.profit(n = n, sd = sd, bake = bake)
}
plot.profit(dist)
data.frame(MeanProfits = round(mean(dist$profit), 0),
stdProfits = round(sqrt(var(dist$profit)), 0),
MeanMissed = round(mean(dist$missed), 0),
stdMissed = round(sqrt(var(dist$missed)), 0) )
}
## How long to simulations take?
system.time(
dist.profit()
)
## Or, get a more detailed, but slower much slower, view
library(microbenchmark)
microbenchmark(
dist.profit(),
times = 20
)
## Test a function
test.demand <- function(){
set.seed(2345)
res <- c("white", "white", "wheat", "white", "white", "white",
"wheat", "multi", "white", "wheat", "white", "white",
"white", "white", "white", "white", "wheat", "white",
"wheat", "white")
demand  <- sim.demand(20)
if(!any(demand != res)) print('sim.demand funciton works!')
else print('ERROR: sim.demand failed')
}
sim.demand = function(lambda, n){
arrivals = rpois(n, lambda)  # Compute realizations of arrivals
demand.mat = matrix(0, n, 3) # Initalize a matrix
i = 1
for (a in arrivals) {
demand.mat[i, ] = t(matrix(table(sim.bread(a)))) # Add one realization to matrix
i = i + 1
}
q
sim.demand = function(lambda, n){
arrivals = rpois(n, lambda)  # Compute realizations of arrivals
demand.mat = matrix(0, n, 3) # Initalize a matrix
i = 1
for (a in arrivals) {
demand.mat[i, ] = t(matrix(table(sim.bread(a)))) # Add one realization to matrix
i = i + 1
}
sim.demand(1,100)
rm(list=ls())
# Clear Console:
cat("\014")
rm(list=ls())
# Clear Console:
cat("\014")
sim.demand = function(lambda, n){
arrivals = rpois(n, lambda)  # Compute realizations of arrivals
demand.mat = matrix(0, n, 3) # Initalize a matrix
i = 1
for (a in arrivals) {
demand.mat[i, ] = t(matrix(table(sim.bread(a)))) # Add one realization to matrix
i = i + 1
}
}
d =  sim.demand(10,100)
install.packages("ggplot2")
rm(list=ls())
# Clear Console:
cat("\014")
g
g
library(ggplot2)
library(MASS)
random_points = mvrnorm(10000, mu=c(0.5,0.5), Sigma=matrix(c(1,0.6,0.6,1), nrow=2))
plot(random_points[,1], random_points[,2], xlim=c(-4,4), ylim=c(-4,4), col=rgb(0,0,0,0.25),
main = 'Draws from a bivariate Normal distribution')
likelihood = function(x,y){
sigma = matrix(c(1,0.6,0.6,1), nrow=2)
mu = c(0.5,0.5)
dist = c(x,y) - mu
value = (1/sqrt(4*pi^2**det(sigma))) * exp((-1/2) * t(dist) %*% ginv(sigma) %*% t(t(dist)) )
return(value)
}
x_chain = 4
y_chain = -4
chain_length = 10000
current_val = likelihood(x_chain,y_chain)
current_val
proposal_sd = .1
accept_count = 0
reject_count = 0
for (n in 1:(chain_length-1)){ # chain length minus 1 because we already have a point (the starting point)
proposed_x = x_chain[n] + rnorm(1, mean=0, sd=proposal_sd)
proposed_y = y_chain[n] + rnorm(1, mean=0, sd=proposal_sd)
proposed_val = likelihood(proposed_x, proposed_y)
# Accept according to probability:
if (runif(1) < (proposed_val/current_val)){
x_chain = c(x_chain, proposed_x)
y_chain = c(y_chain, proposed_y)
current_val = proposed_val
accept_count = accept_count + 1
}else{
x_chain = c(x_chain, x_chain[n])
y_chain = c(y_chain, y_chain[n])
reject_count = reject_count + 1
}
}
plot(x_chain, y_chain, col=rgb(0,0,0,0.25), xlim=c(-4,4), ylim=c(-4,4),
main="MCMC values for a Bivariate Normal", xlab="x", ylab="y")
num_burnin = round(0.1*chain_length)
num_burnin
plot(x_chain[num_burnin:chain_length], y_chain[num_burnin:chain_length],
col=rgb(0,0,0,0.25), xlim=c(-4,4), ylim=c(-4,4),
main="MCMC values for a Bivariate Normal with burn-in", xlab="x", ylab="y")
mcmc_map = c(mean(x_chain), mean(y_chain))
mcmc_map
accept_count/chain_length
reject_count/chain_length
par(mfrow = c(2,1))
plot(x_chain, type="l", main = 'X chain', ylab = 'Value')
plot(y_chain, type="l", main = 'Y chain', ylab = 'Value')
par(mfrow = c(1,1))
par(mfrow = c(2,1))
plot(x_chain[1000:2000], type="l", main = 'X chain', ylab = 'Value')
plot(y_chain[1000:2000], type="l", main = 'Y chain', ylab = 'Value')
par(mfrow = c(1,1))
library(LearnBayes)
minmaxpost <- function(theta, data){
mu <- theta[1]
sigma <- exp(theta[2])
dnorm(data$min, mu, sigma, log=TRUE) +
dnorm(data$max, mu, sigma, log=TRUE) +
(data$n - 2) * log(pnorm(data$max, mu, sigma) -
pnorm(data$min, mu, sigma))
}
data <- list(n=10, min=52, max=84)
fit <- laplace(minmaxpost, c(70, 2), data)
fit
mycontour(minmaxpost, c(45, 95, 1.5, 4), data,
xlab=expression(mu), ylab=expression(paste("log ",sigma)))
mycontour(lbinorm, c(45, 95, 1.5, 4),
list(m=fit$mode, v=fit$var), add=TRUE, col="red",
main = 'Contours of posterior with Normal approx in red')
mcmc.fit <- rwmetrop(minmaxpost,
list(var=fit$v, scale=3),
c(70, 2),
10000,
data)
mcmc.fit$accept  # What is the acceptance ratio
mycontour(minmaxpost, c(45, 95, 1.5, 4), data,
xlab=expression(mu),
ylab=expression(paste("log ",sigma)))
points(mcmc.fit$par)
mu <- mcmc.fit$par[, 1]
sigma <- exp(mcmc.fit$par[, 2])
P.75 <- mu + 0.674 * sigma
plot(density(P.75),
main="Posterior Density of Upper Quartile")
library(LearnBayes)
d <- data.frame(Name=c("Clemente", "Robinson", "Howard", "Johnstone",
"Berry", "Spencer", "Kessinger", "Alvarado", "Santo",
"Swaboda", "Petrocelli", "Rodriguez", "Scott", "Unser",
"Williams", "Campaneris", "Munson", "Alvis"),
Hits=c(18, 17, 16, 15, 14, 14, 13, 12, 11,
11, 10, 10, 10, 10, 10, 9, 8, 7),
At.Bats=45)
laplace.fit <- laplace(betabinexch,
c(0, 0),
d[, c("Hits", "At.Bats")])
# Clear objects from Memory
rm(list=ls())
# Clear Console:
cat("\014")
library(LearnBayes)
d <- data.frame(Name=c("Clemente", "Robinson", "Howard", "Johnstone",
"Berry", "Spencer", "Kessinger", "Alvarado", "Santo",
"Swaboda", "Petrocelli", "Rodriguez", "Scott", "Unser",
"Williams", "Campaneris", "Munson", "Alvis"),
Hits=c(18, 17, 16, 15, 14, 14, 13, 12, 11,
11, 10, 10, 10, 10, 10, 9, 8, 7),
At.Bats=45)
laplace.fit <- laplace(betabinexch,
c(0, 0),
d[, c("Hits", "At.Bats")])
laplace.fit
?betabinexch
mcmc.fit <- rwmetrop(betabinexch,
list(var=laplace.fit$var, scale=2),
c(0, 0),
5000,
d[, c("Hits", "At.Bats")])
mcmc.fit$par[1:20]
mcmc.fit$accept
mycontour(betabinexch, c(-1.5, -0.5, 2, 12),
d[, c("Hits", "At.Bats")],
xlab="Logit ETA", ylab="Log K")
with(mcmc.fit, points(par))
eta <- with(mcmc.fit, exp(par[, 1]) / (1 + exp(par[, 1])))
eta[1:20]
K <- exp(mcmc.fit$par[, 2])
K[1:20]
p.estimate <- function(j, eta, K){
yj <- d[j, "Hits"]
nj <- d[j, "At.Bats"]
p.sim <- rbeta(5000, yj + K * eta, nj - yj + K * (1 - eta))
quantile(p.sim, c(0.05, 0.50, 0.95))
}
E <- t(sapply(1:18, p.estimate, eta, K))
rownames(E) <- d[, "Name"]
round(E, 3)
plot(d$Hits / 45, E[, 2], pch=19,
ylim=c(.15, .40),
xlab="Observed AVG", ylab="True Probability",
main="90 Percent Probability Intervals")
for (j in 1:18) lines(d$Hits[j] / 45 * c(1, 1), E[j, c(1, 3)])
abline(a=0, b=1, col="blue")
abline(h=mean(d$Hits) / 45, col="red")
legend("topleft", legend=c("Individual", "Combined"),
lty=1, col=c("blue", "red"))
require(mlbench)
data(HouseVotes84)
require(ggplot2)
Map(function(x, y)
ggplot(HouseVotes84, aes_string(x)) +
geom_bar() +
facet_grid(. ~ Class) +
ggtitle(y),
list('V1', 'V2', 'V3', 'V4', 'V5'),
list(' handicapped-infants',
'water-project-cost-sharing',
'adoption-of-the-budget-resolution',
'physician-fee-freeze',
'el-salvador-aid'))
data(HouseVotes84)
require(e1071)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
party = predict(model, HouseVotes84[1:10,])
nums = predict(model, HouseVotes84[1:10,], type = "raw")
data.frame(party = HouseVotes84$Class[1:10], predicted = party, Democrat = nums[,1], Republican = nums[,2])
pred <- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)
model <- naiveBayes(Class ~ ., data = HouseVotes84, laplace = 3)
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)
N <- 1000
x <- 1:N
epsilon <- rnorm(N, 0, 1)
y <- x + epsilon
path = 'C:\\Users\\rr657c\\Documents\UW\\Data Science Certificate\\DataScience350\\Lecture9'
full.path = file.path(path, 'example.bug')
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
path = 'C:\\Users\\rr657c\\Documents\\UW\\Data Science Certificate\\DataScience350\\Lecture9'
full.path = file.path(path, 'example.bug')
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
full.path = file.path('C:\\Users\\rr657c\\jags-terminal', 'example.bug')
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
path = 'C:\\Users\\rr657c\\Documents\\UW\\Data Science Certificate\\DataScience350\\Lecture9'
full.path = file.path(path, 'example.bug')
jags.mod.reg <- jags.model('C:\\Users\\rr657c\\jags-terminal',
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
getwd()
path = 'C:\\Users\\rr657c\\Documents\\UW\\Data Science Certificate\\DataScience350\\Lecture9'
full.path = file.path(path, 'example.bug')
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
jags.mod.reg <- jags.model('C:\\Users\\rr657c',
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
jags.mod.reg <- jags.model('C:\\Users\\rr657c\\jags-terminal',
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
path = 'C:\\Users\\rr657c\\Documents\\UW\\Data Science Certificate\\DataScience350\\Lecture9'
full.path = file.path(path, 'example.bug')
jags.mod.reg <- jags.model('C:\\Users\\rr657c\\jags-terminal',
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
jags.mod.reg <- jags.model(full.path,
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
?load
# Clear objects from Memory
rm(list=ls())
# Clear Console:
cat("\014")
require(dplyr)
load("MAJORS1012")
expand.grid()
expand.grid(2,2)
expand.grid(3,3)
expand.grid(0,3)
i <- expand.grid(3,3)
View(i)
View(i)
setwd('C:\\Users\\rr657c\\Documents\\UW\\Data Science Certificate\\DataScience450\\Lesson 2')
rm(list=ls())
# Clear Console:
cat("\014")
# Set repeatable random seed.
set.seed(123)
#Load the dataset.
read.wine = function(file = 'RedWhiteWine.arff'){
## Read the arff file
library(foreign)
wine <- read.arff(file)
wine[complete.cases(wine), ]
}
# Partition the data into test and training data sets.
PartitionExact = function(dataSet, fractionOfTest = 0.3)
{
#  browser()
random <-runif(nrow(dataSet))
quant <- quantile(random,fractionOfTest)
testFlag <- random <= quant
testingData <- dataSet[testFlag, ]
trainingData <- dataSet[!testFlag, ]
dataSetSplit <- list(trainingData=trainingData, testingData=testingData)
}
# Load and cleanse the csv file.
wine = read.wine()
colnames(wine)[1] <- "fixed.acidity"
colnames(wine)[2] <- "volatile.acidity"
colnames(wine)[3] <- "citric.acid"
colnames(wine)[4] <- "residual.sugar"
colnames(wine)[6] <- "free.sulfur.dioxide"
colnames(wine)[7] <- "total.sulfur.dioxide"
#Check the structure of the dataset.
str(wine)
#Adding a new numerica attribute based on the R/W column.
wine$kind <- as.numeric(wine$`R/W` == "R")
str(wine)
#check the data set header
head(wine)
tail(wine)
# Remove R/W column
wine$`R/W` <- NULL
# Remove Quality column
wine$quality <- NULL
lapply(wine, summary)
#Data Exploration
library(reshape2)
library(ggplot2)
ggplot(data = melt(wine), mapping = aes(x = value)) +
geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
#Split dataset into test and training data sets.
WineDataset = PartitionExact(wine)
TestWine <- WineDataset$testingData
TrainWine <-WineDataset$trainingData
nrow(TestWine)
head(TestWine)
nrow(TrainWine)
head(TrainWine)
formula <- kind ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + chlorides + density
formulaOne <- kind ~ chlorides
formulaAll <- kind ~ .
# Classification Tree with rpart
library(rpart)
# grow tree
winetree <- rpart(formula = formula, data = TrainWine,method="class")
winetreeOne <- rpart(formula = formulaOne, data = TrainWine,method="class")
winetreeAll <- rpart(formula = formulaAll, data = TrainWine,method="class")
#Plot tree using rpart.plot
library("rpart.plot")
rpart.plot(winetree)
rpart.plot(winetreeOne)
rpart.plot(winetreeAll)
#Use the model to make the predictions.
kindPrediction <- as.numeric(predict(winetree, newdata = TestWine, type="class"))
kindPredictionOne <- as.numeric(predict(winetreeOne, newdata = TestWine, type="class"))
kindPredictionAll <- as.numeric(predict(winetreeAll, newdata = TestWine, type="class"))
#Calculate Confusion Matrix with six attributes.
crosstab <- table(kindPrediction, TestWine$kind)
crosstab
#Calculate Confusion Matrix for model with one attribute
crosstabOne <- table(kindPredictionOne, TestWine$kind)
crosstabOne
#Calculate Confusion Matrix for all atributes.
crosstabAll <- table(kindPredictionAll, TestWine$kind)
crosstabAll
#Calculate Specificity (TPR)
Specificity <- function(Table.X)
{
Specificity.X <- (Table.X[2,2])/(Table.X[1,2]+Table.X[2,2])
return(Specificity.X)
}
# Calculate TPR for all attributes
TPRAll <- Specificity(crosstabAll)
TPRAll
# Calculate TPR for selected attributes
TPR <- Specificity(crosstab)
TPR
# Calculate TPR for one attributes
TPROne <- Specificity(crosstabOne)
TPROne
#calculate FPR from Confusion MAtrix
FPR <- function(Table.X)
{
FPR.X <- (Table.X[1,2])/(Table.X[1,2]+Table.X[2,2])
return(FPR.X)
}
# Calculate FPR for all attributes
FPRAll <- FPR(crosstabAll)
FPRAll
# Calculate FPR for six attributes
FPRSix <- FPR(crosstab)
FPRSix
# Calculate FPR for one attributes
FPROne <- FPR(crosstabOne)
FPROne
#Calculate AUC
library(pROC)
roc_obj <- roc(TestWine$kind, kindPrediction)
auc(roc_obj)
roc_obj_All <- roc(TestWine$kind, kindPredictionAll)
auc(roc_obj_All)
roc_obj_One <- roc(TestWine$kind, kindPredictionOne)
auc(roc_obj_One)
FPRCheck <- 1- roc_obj$sensitivities
TPRCheck <- roc_obj$specificities
head(kindPrediction)
ggplot(data = melt(wine), mapping = aes(x = value)) +
geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
